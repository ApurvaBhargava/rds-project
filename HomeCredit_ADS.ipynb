{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSeeUzEyFjvv"
   },
   "source": [
    "#Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7465,
     "status": "ok",
     "timestamp": 1585933161566,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 240
    },
    "id": "gR__E7qBTRHS",
    "outputId": "b2698643-be4b-4fe5-e0bb-d7281611f13e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KAGGLE HOME CREDIT DEFAULT RISK COMPETITION\n",
    "Adapted from one of the models used in 7th place solution ensemble.\n",
    "For more details about our solution please check this discussion:\n",
    "https://www.kaggle.com/c/home-credit-default-risk/discussion/64580\n",
    "\n",
    "Another similar version is also available at GitHub:\n",
    "https://github.com/js-aguiar/home-credit-default-competition\n",
    "\n",
    "This model uses LightGBM with goss and label encode for the application's \n",
    "categorical features. Other tables are using one-hot encode with mean, \n",
    "sum and a few different functions to aggregate. The main ideia was to add \n",
    "more time related features like last application and last X months aggregations.\n",
    "There are also aggregations for specific loan types and status as well as\n",
    "ratios between tables. Configurations are in line 785\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from scipy.stats import kurtosis, iqr, skew\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def main(debug= False):\n",
    "    data_path = \"/content/drive/My Drive/2020 Spring/RDS/rds-project/data\"\n",
    "    output_path = \"/content/drive/My Drive/2020 Spring/RDS/rds-project/output\"\n",
    "    num_rows = 30000 if debug else None\n",
    "    # with timer(\"application_train and application_test\"):\n",
    "    #     df = get_train_test(DATA_DIRECTORY, num_rows= num_rows)\n",
    "    #     print(\"Application dataframe shape: \", df.shape)\n",
    "    # with timer(\"Bureau and bureau_balance data\"):\n",
    "    #     bureau_df = get_bureau(DATA_DIRECTORY, num_rows= num_rows)\n",
    "    #     df = pd.merge(df, bureau_df, on='SK_ID_CURR', how='left')\n",
    "    #     print(\"Bureau dataframe shape: \", bureau_df.shape)\n",
    "    #     del bureau_df; gc.collect()\n",
    "    # with timer(\"previous_application\"):\n",
    "    #     prev_df = get_previous_applications(DATA_DIRECTORY, num_rows)\n",
    "    #     df = pd.merge(df, prev_df, on='SK_ID_CURR', how='left')\n",
    "    #     print(\"Previous dataframe shape: \", prev_df.shape)\n",
    "    #     del prev_df; gc.collect()\n",
    "    # with timer(\"previous applications balances\"):\n",
    "    #     pos = get_pos_cash(DATA_DIRECTORY, num_rows)\n",
    "    #     df = pd.merge(df, pos, on='SK_ID_CURR', how='left')\n",
    "    #     print(\"Pos-cash dataframe shape: \", pos.shape)\n",
    "    #     del pos; gc.collect()\n",
    "    #     ins = get_installment_payments(DATA_DIRECTORY, num_rows)\n",
    "    #     df = pd.merge(df, ins, on='SK_ID_CURR', how='left')\n",
    "    #     print(\"Installments dataframe shape: \", ins.shape)\n",
    "    #     del ins; gc.collect()\n",
    "    #     cc = get_credit_card(DATA_DIRECTORY, num_rows)\n",
    "    #     df = pd.merge(df, cc, on='SK_ID_CURR', how='left')\n",
    "    #     print(\"Credit card dataframe shape: \", cc.shape)\n",
    "    #     del cc; gc.collect()\n",
    "    # # Add ratios and groupby between different tables\n",
    "    # df = add_ratios_features(df)\n",
    "    # df = reduce_memory(df)\n",
    "    # df.to_csv(os.path.join(data_path, \"engineered_df.csv\"))\n",
    "    with timer(\"Loading engineered features\"):\n",
    "        df = pd.read_csv(os.path.join(data_path, \"engineered_df.csv\"))\n",
    "        print(f\"Engineered df shape: {df.shape}\")\n",
    "    lgbm_categorical_feat = [\n",
    "        'CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE',\n",
    "        'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE',\n",
    "        'ORGANIZATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'NAME_TYPE_SUITE', 'WALLSMATERIAL_MODE']\n",
    "    with timer(\"Run LightGBM\"):\n",
    "        feat_importance = kfold_lightgbm_sklearn(df, lgbm_categorical_feat)\n",
    "        print(feat_importance)\n",
    "\n",
    "\n",
    "def add_ratios_features(df):\n",
    "    # CREDIT TO INCOME RATIO\n",
    "    df['BUREAU_INCOME_CREDIT_RATIO'] = df['BUREAU_AMT_CREDIT_SUM_MEAN'] / df['AMT_INCOME_TOTAL']\n",
    "    df['BUREAU_ACTIVE_CREDIT_TO_INCOME_RATIO'] = df['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_INCOME_TOTAL']\n",
    "    # PREVIOUS TO CURRENT CREDIT RATIO\n",
    "    df['CURRENT_TO_APPROVED_CREDIT_MIN_RATIO'] = df['APPROVED_AMT_CREDIT_MIN'] / df['AMT_CREDIT']\n",
    "    df['CURRENT_TO_APPROVED_CREDIT_MAX_RATIO'] = df['APPROVED_AMT_CREDIT_MAX'] / df['AMT_CREDIT']\n",
    "    df['CURRENT_TO_APPROVED_CREDIT_MEAN_RATIO'] = df['APPROVED_AMT_CREDIT_MEAN'] / df['AMT_CREDIT']\n",
    "    # PREVIOUS TO CURRENT ANNUITY RATIO\n",
    "    df['CURRENT_TO_APPROVED_ANNUITY_MAX_RATIO'] = df['APPROVED_AMT_ANNUITY_MAX'] / df['AMT_ANNUITY']\n",
    "    df['CURRENT_TO_APPROVED_ANNUITY_MEAN_RATIO'] = df['APPROVED_AMT_ANNUITY_MEAN'] / df['AMT_ANNUITY']\n",
    "    df['PAYMENT_MIN_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MIN'] / df['AMT_ANNUITY']\n",
    "    df['PAYMENT_MAX_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MAX'] / df['AMT_ANNUITY']\n",
    "    df['PAYMENT_MEAN_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MEAN'] / df['AMT_ANNUITY']\n",
    "    # PREVIOUS TO CURRENT CREDIT TO ANNUITY RATIO\n",
    "    df['CTA_CREDIT_TO_ANNUITY_MAX_RATIO'] = df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MAX'] / df[\n",
    "        'CREDIT_TO_ANNUITY_RATIO']\n",
    "    df['CTA_CREDIT_TO_ANNUITY_MEAN_RATIO'] = df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MEAN'] / df[\n",
    "        'CREDIT_TO_ANNUITY_RATIO']\n",
    "    # DAYS DIFFERENCES AND RATIOS\n",
    "    df['DAYS_DECISION_MEAN_TO_BIRTH'] = df['APPROVED_DAYS_DECISION_MEAN'] / df['DAYS_BIRTH']\n",
    "    df['DAYS_CREDIT_MEAN_TO_BIRTH'] = df['BUREAU_DAYS_CREDIT_MEAN'] / df['DAYS_BIRTH']\n",
    "    df['DAYS_DECISION_MEAN_TO_EMPLOYED'] = df['APPROVED_DAYS_DECISION_MEAN'] / df['DAYS_EMPLOYED']\n",
    "    df['DAYS_CREDIT_MEAN_TO_EMPLOYED'] = df['BUREAU_DAYS_CREDIT_MEAN'] / df['DAYS_EMPLOYED']\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------- LIGHTGBM MODEL -------------------------\n",
    "\n",
    "def kfold_lightgbm_sklearn(data, categorical_feature = None):\n",
    "    df = data[data['TARGET'].notnull()]\n",
    "    test = data[data['TARGET'].isnull()]\n",
    "    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n",
    "    del_features = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n",
    "    predictors = list(filter(lambda v: v not in del_features, df.columns))\n",
    "\n",
    "    if not STRATIFIED_KFOLD:\n",
    "        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
    "    else:\n",
    "        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
    "\n",
    "    # Hold oof predictions, test predictions, feature importance and training/valid auc\n",
    "    oof_preds = np.zeros(df.shape[0])\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    importance_df = pd.DataFrame()\n",
    "    eval_results = dict()\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['TARGET'])):\n",
    "        train_x, train_y = df[predictors].iloc[train_idx], df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = df[predictors].iloc[valid_idx], df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n",
    "        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n",
    "        if not categorical_feature:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                    eval_metric='auc', verbose=400, early_stopping_rounds= EARLY_STOPPING)\n",
    "        else:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                    eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING,\n",
    "                    feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        # Feature importance by GAIN and SPLIT\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = predictors\n",
    "        fold_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance[\"split\"] = clf.booster_.feature_importance(importance_type='split')\n",
    "        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n",
    "        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n",
    "        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n",
    "\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        joblib.dump(clf, os.path.join(output_path, \"n_fold_\"+str(n_fold)+\".pkl\"))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(df['TARGET'], oof_preds))\n",
    "    test['TARGET'] = sub_preds.copy()\n",
    "\n",
    "    # Get the average feature importance between folds\n",
    "    mean_importance = importance_df.groupby('feature').mean().reset_index()\n",
    "    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n",
    "    # Save feature importance, test predictions and oof predictions as csv\n",
    "    if GENERATE_SUBMISSION_FILES:\n",
    "\n",
    "        # Generate oof csv\n",
    "        oof = pd.DataFrame()\n",
    "        oof['SK_ID_CURR'] = df['SK_ID_CURR'].copy()\n",
    "        df['PREDICTIONS'] = oof_preds.copy()\n",
    "        df['TARGET'] = df['TARGET'].copy()\n",
    "        df.to_csv(os.path.join(output_path, 'oof{}.csv'.format(SUBMISSION_SUFIX)), index=False)\n",
    "        # Save submission (test data) and feature importance\n",
    "        test[['SK_ID_CURR', 'TARGET']].to_csv('submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        mean_importance.to_csv(os.path.join(output_path, 'feature_importance{}.csv'.format(SUBMISSION_SUFIX)), index=False)\n",
    "    return mean_importance\n",
    "\n",
    "\n",
    "# ------------------------- APPLICATION PIPELINE -------------------------\n",
    "\n",
    "def get_train_test(path, num_rows = None):\n",
    "    \"\"\" Process application_train.csv and application_test.csv and return a pandas dataframe. \"\"\"\n",
    "    train = pd.read_csv(os.path.join(path, 'application_train.csv'), nrows= num_rows)\n",
    "    test = pd.read_csv(os.path.join(path, 'application_test.csv'), nrows= num_rows)\n",
    "    df = train.append(test)\n",
    "    del train, test; gc.collect()\n",
    "    # Data cleaning\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']  # 4 people with XNA code gender\n",
    "    df = df[df['AMT_INCOME_TOTAL'] < 20000000]  # Max income in test is 4M; train has a 117M value\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Flag_document features - count and kurtosis\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    # Categorical age - based on target=1 plot\n",
    "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "\n",
    "    # New features based on External sources\n",
    "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "    # Credit ratios\n",
    "    df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    # Income ratios\n",
    "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    # Time ratios\n",
    "    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # Groupby: Statistics for applications in the same group\n",
    "    group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
    "    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
    "    df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
    "    df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
    "    df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
    "    df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
    "    df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
    "    df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
    "    df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
    "    df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')\n",
    "\n",
    "    # Encode categorical features (LabelEncoder)\n",
    "    df, le_encoded_cols = label_encoder(df, None)\n",
    "    df = drop_application_columns(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_application_columns(df):\n",
    "    \"\"\" Drop features based on permutation feature importance. \"\"\"\n",
    "    drop_list = [\n",
    "        'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n",
    "        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
    "        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n",
    "        'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n",
    "        'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n",
    "        'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n",
    "        'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n",
    "        'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n",
    "        'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n",
    "        'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n",
    "        'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n",
    "        'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n",
    "        'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE'\n",
    "    ]\n",
    "    # Drop most flag document columns\n",
    "    for doc_num in [2,4,5,6,7,9,10,11,12,13,14,15,16,17,19,20,21]:\n",
    "        drop_list.append('FLAG_DOCUMENT_{}'.format(doc_num))\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_age_label(days_birth):\n",
    "    \"\"\" Return the age group label (int). \"\"\"\n",
    "    age_years = -days_birth / 365\n",
    "    if age_years < 27: return 1\n",
    "    elif age_years < 40: return 2\n",
    "    elif age_years < 50: return 3\n",
    "    elif age_years < 65: return 4\n",
    "    elif age_years < 99: return 5\n",
    "    else: return 0\n",
    "\n",
    "\n",
    "# ------------------------- BUREAU PIPELINE -------------------------\n",
    "\n",
    "def get_bureau(path, num_rows= None):\n",
    "    \"\"\" Process bureau.csv and bureau_balance.csv and return a pandas dataframe. \"\"\"\n",
    "    bureau = pd.read_csv(os.path.join(path, 'bureau.csv'), nrows= num_rows)\n",
    "    # Credit duration and credit/account end date difference\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    # Credit to debt ratio and difference\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
    "\n",
    "    # One-hot encoder\n",
    "    bureau, categorical_cols = one_hot_encoder(bureau, nan_as_category= False)\n",
    "    # Join bureau balance features\n",
    "    bureau = bureau.merge(get_bureau_balance(path, num_rows), how='left', on='SK_ID_BUREAU')\n",
    "    # Flag months with late payments (days past due)\n",
    "    bureau['STATUS_12345'] = 0\n",
    "    for i in range(1,6):\n",
    "        bureau['STATUS_12345'] += bureau['STATUS_{}'.format(i)]\n",
    "\n",
    "    # Aggregate by number of months in balance and merge with bureau (loan length agg)\n",
    "    features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM',\n",
    "        'AMT_CREDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n",
    "    agg_length = bureau.groupby('MONTHS_BALANCE_SIZE')[features].mean().reset_index()\n",
    "    agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)\n",
    "    bureau = bureau.merge(agg_length, how='left', on='MONTHS_BALANCE_SIZE')\n",
    "    del agg_length; gc.collect()\n",
    "\n",
    "    # General loans aggregations\n",
    "    agg_bureau = group(bureau, 'BUREAU_', BUREAU_AGG)\n",
    "    # Active and closed loans aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    agg_bureau = group_and_merge(active,agg_bureau,'BUREAU_ACTIVE_',BUREAU_ACTIVE_AGG)\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    agg_bureau = group_and_merge(closed,agg_bureau,'BUREAU_CLOSED_',BUREAU_CLOSED_AGG)\n",
    "    del active, closed; gc.collect()\n",
    "    # Aggregations for the main loan types\n",
    "    for credit_type in ['Consumer credit', 'Credit card', 'Mortgage', 'Car loan', 'Microloan']:\n",
    "        type_df = bureau[bureau['CREDIT_TYPE_' + credit_type] == 1]\n",
    "        prefix = 'BUREAU_' + credit_type.split(' ')[0].upper() + '_'\n",
    "        agg_bureau = group_and_merge(type_df, agg_bureau, prefix, BUREAU_LOAN_TYPE_AGG)\n",
    "        del type_df; gc.collect()\n",
    "    # Time based aggregations: last x months\n",
    "    for time_frame in [6, 12]:\n",
    "        prefix = \"BUREAU_LAST{}M_\".format(time_frame)\n",
    "        time_frame_df = bureau[bureau['DAYS_CREDIT'] >= -30*time_frame]\n",
    "        agg_bureau = group_and_merge(time_frame_df, agg_bureau, prefix, BUREAU_TIME_AGG)\n",
    "        del time_frame_df; gc.collect()\n",
    "\n",
    "    # Last loan max overdue\n",
    "    sort_bureau = bureau.sort_values(by=['DAYS_CREDIT'])\n",
    "    gr = sort_bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].last().reset_index()\n",
    "    gr.rename({'AMT_CREDIT_MAX_OVERDUE': 'BUREAU_LAST_LOAN_MAX_OVERDUE'}, inplace=True)\n",
    "    agg_bureau = agg_bureau.merge(gr, on='SK_ID_CURR', how='left')\n",
    "    # Ratios: total debt/total credit and active loans debt/ active loans credit\n",
    "    agg_bureau['BUREAU_DEBT_OVER_CREDIT'] = \\\n",
    "        agg_bureau['BUREAU_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_AMT_CREDIT_SUM_SUM']\n",
    "    agg_bureau['BUREAU_ACTIVE_DEBT_OVER_CREDIT'] = \\\n",
    "        agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM']\n",
    "    return agg_bureau\n",
    "\n",
    "\n",
    "def get_bureau_balance(path, num_rows= None):\n",
    "    bb = pd.read_csv(os.path.join(path, 'bureau_balance.csv'), nrows= num_rows)\n",
    "    bb, categorical_cols = one_hot_encoder(bb, nan_as_category= False)\n",
    "    # Calculate rate for each category with decay\n",
    "    bb_processed = bb.groupby('SK_ID_BUREAU')[categorical_cols].mean().reset_index()\n",
    "    # Min, Max, Count and mean duration of payments (months)\n",
    "    agg = {'MONTHS_BALANCE': ['min', 'max', 'mean', 'size']}\n",
    "    bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n",
    "    del bb; gc.collect()\n",
    "    return bb_processed\n",
    "\n",
    "# ------------------------- PREVIOUS PIPELINE -------------------------\n",
    "\n",
    "def get_previous_applications(path, num_rows= None):\n",
    "    \"\"\" Process previous_application.csv and return a pandas dataframe. \"\"\"\n",
    "    prev = pd.read_csv(os.path.join(path, 'previous_application.csv'), nrows= num_rows)\n",
    "    pay = pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows= num_rows)\n",
    "\n",
    "    # One-hot encode most important categorical features\n",
    "    ohe_columns = [\n",
    "        'NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n",
    "        'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n",
    "        'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n",
    "    prev, categorical_cols = one_hot_encoder(prev, ohe_columns, nan_as_category= False)\n",
    "\n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT']/prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n",
    "\n",
    "    # Active loans - approved and not complete yet (last_due 365243)\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    active_df = approved[approved['DAYS_LAST_DUE'] == 365243]\n",
    "    # Find how much was already payed in active loans (using installments csv)\n",
    "    active_pay = pay[pay['SK_ID_PREV'].isin(active_df['SK_ID_PREV'])]\n",
    "    active_pay_agg = active_pay.groupby('SK_ID_PREV')[['AMT_INSTALMENT', 'AMT_PAYMENT']].sum()\n",
    "    active_pay_agg.reset_index(inplace= True)\n",
    "    # Active loans: difference of what was payed and installments\n",
    "    active_pay_agg['INSTALMENT_PAYMENT_DIFF'] = active_pay_agg['AMT_INSTALMENT'] - active_pay_agg['AMT_PAYMENT']\n",
    "    # Merge with active_df\n",
    "    active_df = active_df.merge(active_pay_agg, on= 'SK_ID_PREV', how= 'left')\n",
    "    active_df['REMAINING_DEBT'] = active_df['AMT_CREDIT'] - active_df['AMT_PAYMENT']\n",
    "    active_df['REPAYMENT_RATIO'] = active_df['AMT_PAYMENT'] / active_df['AMT_CREDIT']\n",
    "    # Perform aggregations for active applications\n",
    "    active_agg_df = group(active_df, 'PREV_ACTIVE_', PREVIOUS_ACTIVE_AGG)\n",
    "    active_agg_df['TOTAL_REPAYMENT_RATIO'] = active_agg_df['PREV_ACTIVE_AMT_PAYMENT_SUM']/\\\n",
    "                                             active_agg_df['PREV_ACTIVE_AMT_CREDIT_SUM']\n",
    "    del active_pay, active_pay_agg, active_df; gc.collect()\n",
    "\n",
    "    # Change 365.243 values to nan (missing)\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Days last due difference (scheduled x done)\n",
    "    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "    approved['DAYS_LAST_DUE_DIFF'] = approved['DAYS_LAST_DUE_1ST_VERSION'] - approved['DAYS_LAST_DUE']\n",
    "\n",
    "    # Categorical features\n",
    "    categorical_agg = {key: ['mean'] for key in categorical_cols}\n",
    "    # Perform general aggregations\n",
    "    agg_prev = group(prev, 'PREV_', {**PREVIOUS_AGG, **categorical_agg})\n",
    "    # Merge active loans dataframe on agg_prev\n",
    "    agg_prev = agg_prev.merge(active_agg_df, how='left', on='SK_ID_CURR')\n",
    "    del active_agg_df; gc.collect()\n",
    "    # Aggregations for approved and refused loans\n",
    "    agg_prev = group_and_merge(approved, agg_prev, 'APPROVED_', PREVIOUS_APPROVED_AGG)\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    agg_prev = group_and_merge(refused, agg_prev, 'REFUSED_', PREVIOUS_REFUSED_AGG)\n",
    "    del approved, refused; gc.collect()\n",
    "    # Aggregations for Consumer loans and Cash loans\n",
    "    for loan_type in ['Consumer loans', 'Cash loans']:\n",
    "        type_df = prev[prev['NAME_CONTRACT_TYPE_{}'.format(loan_type)] == 1]\n",
    "        prefix = 'PREV_' + loan_type.split(\" \")[0] + '_'\n",
    "        agg_prev = group_and_merge(type_df, agg_prev, prefix, PREVIOUS_LOAN_TYPE_AGG)\n",
    "        del type_df; gc.collect()\n",
    "\n",
    "    # Get the SK_ID_PREV for loans with late payments (days past due)\n",
    "    pay['LATE_PAYMENT'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n",
    "    pay['LATE_PAYMENT'] = pay['LATE_PAYMENT'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    dpd_id = pay[pay['LATE_PAYMENT'] > 0]['SK_ID_PREV'].unique()\n",
    "    # Aggregations for loans with late payments\n",
    "    agg_dpd = group_and_merge(prev[prev['SK_ID_PREV'].isin(dpd_id)], agg_prev,\n",
    "                                    'PREV_LATE_', PREVIOUS_LATE_PAYMENTS_AGG)\n",
    "    del agg_dpd, dpd_id; gc.collect()\n",
    "    # Aggregations for loans in the last x months\n",
    "    for time_frame in [12, 24]:\n",
    "        time_frame_df = prev[prev['DAYS_DECISION'] >= -30*time_frame]\n",
    "        prefix = 'PREV_LAST{}M_'.format(time_frame)\n",
    "        agg_prev = group_and_merge(time_frame_df, agg_prev, prefix, PREVIOUS_TIME_AGG)\n",
    "        del time_frame_df; gc.collect()\n",
    "    del prev; gc.collect()\n",
    "    return agg_prev\n",
    "\n",
    "# ------------------------- POS-CASH PIPELINE -------------------------\n",
    "\n",
    "def get_pos_cash(path, num_rows= None):\n",
    "    \"\"\" Process POS_CASH_balance.csv and return a pandas dataframe. \"\"\"\n",
    "    pos = pd.read_csv(os.path.join(path, 'POS_CASH_balance.csv'), nrows= num_rows)\n",
    "    pos, categorical_cols = one_hot_encoder(pos, nan_as_category= False)\n",
    "    # Flag months with late payment\n",
    "    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # Aggregate by SK_ID_CURR\n",
    "    categorical_agg = {key: ['mean'] for key in categorical_cols}\n",
    "    pos_agg = group(pos, 'POS_', {**POS_CASH_AGG, **categorical_agg})\n",
    "    # Sort and group by SK_ID_PREV\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df = pd.DataFrame()\n",
    "    df['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df['POS_COMPLETED_BEFORE_MEAN'] = df.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0\n",
    "                                                and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "    del df, gp, df_gp, sort_pos; gc.collect()\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR','LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Drop some useless categorical features\n",
    "    drop_features = [\n",
    "        'POS_NAME_CONTRACT_STATUS_Canceled_MEAN', 'POS_NAME_CONTRACT_STATUS_Amortized debt_MEAN',\n",
    "        'POS_NAME_CONTRACT_STATUS_XNA_MEAN']\n",
    "    pos_agg.drop(drop_features, axis=1, inplace=True)\n",
    "    return pos_agg\n",
    "\n",
    "# ------------------------- INSTALLMENTS PIPELINE -------------------------\n",
    "\n",
    "def get_installment_payments(path, num_rows= None):\n",
    "    \"\"\" Process installments_payments.csv and return a pandas dataframe. \"\"\"\n",
    "    pay = pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows= num_rows)\n",
    "    # Group payments and get Payment difference\n",
    "    pay = do_sum(pay, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    pay['PAYMENT_DIFFERENCE'] = pay['AMT_INSTALMENT'] - pay['AMT_PAYMENT_GROUPED']\n",
    "    pay['PAYMENT_RATIO'] = pay['AMT_INSTALMENT'] / pay['AMT_PAYMENT_GROUPED']\n",
    "    pay['PAID_OVER_AMOUNT'] = pay['AMT_PAYMENT'] - pay['AMT_INSTALMENT']\n",
    "    pay['PAID_OVER'] = (pay['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "    # Payment Entry: Days past due and Days before due\n",
    "    pay['DPD'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n",
    "    pay['DPD'] = pay['DPD'].apply(lambda x: 0 if x <= 0 else x)\n",
    "    pay['DBD'] = pay['DAYS_INSTALMENT'] - pay['DAYS_ENTRY_PAYMENT']\n",
    "    pay['DBD'] = pay['DBD'].apply(lambda x: 0 if x <= 0 else x)\n",
    "    # Flag late payment\n",
    "    pay['LATE_PAYMENT'] = pay['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # Percentage of payments that were late\n",
    "    pay['INSTALMENT_PAYMENT_RATIO'] = pay['AMT_PAYMENT'] / pay['AMT_INSTALMENT']\n",
    "    pay['LATE_PAYMENT_RATIO'] = pay.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "    # Flag late payments that have a significant amount\n",
    "    pay['SIGNIFICANT_LATE_PAYMENT'] = pay['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    # Flag k threshold late payments\n",
    "    pay['DPD_7'] = pay['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    pay['DPD_15'] = pay['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "    # Aggregations by SK_ID_CURR\n",
    "    pay_agg = group(pay, 'INS_', INSTALLMENTS_AGG)\n",
    "\n",
    "    # Installments in the last x months\n",
    "    for months in [36, 60]:\n",
    "        recent_prev_id = pay[pay['DAYS_INSTALMENT'] >= -30*months]['SK_ID_PREV'].unique()\n",
    "        pay_recent = pay[pay['SK_ID_PREV'].isin(recent_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        pay_agg = group_and_merge(pay_recent, pay_agg, prefix, INSTALLMENTS_TIME_AGG)\n",
    "\n",
    "    # Last x periods trend features\n",
    "    group_features = ['SK_ID_CURR', 'SK_ID_PREV', 'DPD', 'LATE_PAYMENT',\n",
    "                      'PAID_OVER_AMOUNT', 'PAID_OVER', 'DAYS_INSTALMENT']\n",
    "    gp = pay[group_features].groupby('SK_ID_CURR')\n",
    "    func = partial(trend_in_last_k_instalment_features, periods= INSTALLMENTS_LAST_K_TREND_PERIODS)\n",
    "    g = parallel_apply(gp, func, index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n",
    "    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Last loan features\n",
    "    g = parallel_apply(gp, installments_last_loan_features, index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n",
    "    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n",
    "    return pay_agg\n",
    "\n",
    "\n",
    "def trend_in_last_k_instalment_features(gr, periods):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n",
    "    features = {}\n",
    "\n",
    "    for period in periods:\n",
    "        gr_period = gr_.iloc[:period]\n",
    "        features = add_trend_feature(features, gr_period, 'DPD',\n",
    "                                           '{}_TREND_'.format(period))\n",
    "        features = add_trend_feature(features, gr_period, 'PAID_OVER_AMOUNT',\n",
    "                                           '{}_TREND_'.format(period))\n",
    "    return features\n",
    "\n",
    "\n",
    "def installments_last_loan_features(gr):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n",
    "    last_installment_id = gr_['SK_ID_PREV'].iloc[0]\n",
    "    gr_ = gr_[gr_['SK_ID_PREV'] == last_installment_id]\n",
    "\n",
    "    features = {}\n",
    "    features = add_features_in_group(features, gr_, 'DPD',\n",
    "                                     ['sum', 'mean', 'max', 'std'],\n",
    "                                     'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'LATE_PAYMENT',\n",
    "                                     ['count', 'mean'],\n",
    "                                     'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'PAID_OVER_AMOUNT',\n",
    "                                     ['sum', 'mean', 'max', 'min', 'std'],\n",
    "                                     'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'PAID_OVER',\n",
    "                                     ['count', 'mean'],\n",
    "                                     'LAST_LOAN_')\n",
    "    return features\n",
    "\n",
    "# ------------------------- CREDIT CARD PIPELINE -------------------------\n",
    "\n",
    "def get_credit_card(path, num_rows= None):\n",
    "    \"\"\" Process credit_card_balance.csv and return a pandas dataframe. \"\"\"\n",
    "    cc = pd.read_csv(os.path.join(path, 'credit_card_balance.csv'), nrows= num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=False)\n",
    "    cc.rename(columns={'AMT_RECIVABLE': 'AMT_RECEIVABLE'}, inplace=True)\n",
    "    # Amount used from limit\n",
    "    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Current payment / Min payment\n",
    "    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "    # Late payment\n",
    "    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # How much drawing of limit\n",
    "    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Aggregations by SK_ID_CURR\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(CREDIT_CARD_AGG)\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    cc_agg.reset_index(inplace= True)\n",
    "\n",
    "    # Last month balance of each credit card application\n",
    "    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = cc[cc.index.isin(last_ids)]\n",
    "    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    # Aggregations for last x months\n",
    "    for months in [12, 24, 48]:\n",
    "        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "    return cc_agg\n",
    "\n",
    "\n",
    "# ------------------------- UTILITY FUNCTIONS -------------------------\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n",
    "\n",
    "\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
    "\n",
    "\n",
    "def do_mean(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_median(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_std(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_sum(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
    "    \"\"\"Create a new column for each categorical value in categorical columns. \"\"\"\n",
    "    original_columns = list(df.columns)\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, categorical_columns\n",
    "\n",
    "\n",
    "def label_encoder(df, categorical_columns=None):\n",
    "    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for col in categorical_columns:\n",
    "        df[col], uniques = pd.factorize(df[col])\n",
    "    return df, categorical_columns\n",
    "\n",
    "\n",
    "def add_features(feature_name, aggs, features, feature_names, groupby):\n",
    "    feature_names.extend(['{}_{}'.format(feature_name, agg) for agg in aggs])\n",
    "\n",
    "    for agg in aggs:\n",
    "        if agg == 'kurt':\n",
    "            agg_func = kurtosis\n",
    "        elif agg == 'iqr':\n",
    "            agg_func = iqr\n",
    "        else:\n",
    "            agg_func = agg\n",
    "\n",
    "        g = groupby[feature_name].agg(agg_func).reset_index().rename(index=str,\n",
    "                                                                     columns={feature_name: '{}_{}'.format(feature_name,agg)})\n",
    "        features = features.merge(g, on='SK_ID_CURR', how='left')\n",
    "    return features, feature_names\n",
    "\n",
    "\n",
    "def add_features_in_group(features, gr_, feature_name, aggs, prefix):\n",
    "    for agg in aggs:\n",
    "        if agg == 'sum':\n",
    "            features['{}{}_sum'.format(prefix, feature_name)] = gr_[feature_name].sum()\n",
    "        elif agg == 'mean':\n",
    "            features['{}{}_mean'.format(prefix, feature_name)] = gr_[feature_name].mean()\n",
    "        elif agg == 'max':\n",
    "            features['{}{}_max'.format(prefix, feature_name)] = gr_[feature_name].max()\n",
    "        elif agg == 'min':\n",
    "            features['{}{}_min'.format(prefix, feature_name)] = gr_[feature_name].min()\n",
    "        elif agg == 'std':\n",
    "            features['{}{}_std'.format(prefix, feature_name)] = gr_[feature_name].std()\n",
    "        elif agg == 'count':\n",
    "            features['{}{}_count'.format(prefix, feature_name)] = gr_[feature_name].count()\n",
    "        elif agg == 'skew':\n",
    "            features['{}{}_skew'.format(prefix, feature_name)] = skew(gr_[feature_name])\n",
    "        elif agg == 'kurt':\n",
    "            features['{}{}_kurt'.format(prefix, feature_name)] = kurtosis(gr_[feature_name])\n",
    "        elif agg == 'iqr':\n",
    "            features['{}{}_iqr'.format(prefix, feature_name)] = iqr(gr_[feature_name])\n",
    "        elif agg == 'median':\n",
    "            features['{}{}_median'.format(prefix, feature_name)] = gr_[feature_name].median()\n",
    "    return features\n",
    "\n",
    "\n",
    "def add_trend_feature(features, gr, feature_name, prefix):\n",
    "    y = gr[feature_name].values\n",
    "    try:\n",
    "        x = np.arange(0, len(y)).reshape(-1, 1)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(x, y)\n",
    "        trend = lr.coef_[0]\n",
    "    except:\n",
    "        trend = np.nan\n",
    "    features['{}{}'.format(prefix, feature_name)] = trend\n",
    "    return features\n",
    "\n",
    "\n",
    "def parallel_apply(groups, func, index_name='Index', num_workers=0, chunk_size=100000):\n",
    "    if num_workers <= 0: num_workers = NUM_THREADS\n",
    "    #n_chunks = np.ceil(1.0 * groups.ngroups / chunk_size)\n",
    "    indeces, features = [], []\n",
    "    for index_chunk, groups_chunk in chunk_groups(groups, chunk_size):\n",
    "        with mp.pool.Pool(num_workers) as executor:\n",
    "            features_chunk = executor.map(func, groups_chunk)\n",
    "        features.extend(features_chunk)\n",
    "        indeces.extend(index_chunk)\n",
    "\n",
    "    features = pd.DataFrame(features)\n",
    "    features.index = indeces\n",
    "    features.index.name = index_name\n",
    "    return features\n",
    "\n",
    "\n",
    "def chunk_groups(groupby_object, chunk_size):\n",
    "    n_groups = groupby_object.ngroups\n",
    "    group_chunk, index_chunk = [], []\n",
    "    for i, (index, df) in enumerate(groupby_object):\n",
    "        group_chunk.append(df)\n",
    "        index_chunk.append(index)\n",
    "        if (i + 1) % chunk_size == 0 or i + 1 == n_groups:\n",
    "            group_chunk_, index_chunk_ = group_chunk.copy(), index_chunk.copy()\n",
    "            group_chunk, index_chunk = [], []\n",
    "            yield index_chunk_, group_chunk_\n",
    "\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
    "          .format(start_mem, len(df.columns)))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Can use unsigned int here too\n",
    "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJFBPuZhFusU"
   },
   "source": [
    "#Config Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZPiiEtaUfWH"
   },
   "outputs": [],
   "source": [
    "# ------------------------- CONFIGURATIONS -------------------------\n",
    "# GENERAL CONFIGURATIONS\n",
    "NUM_THREADS = 4\n",
    "DATA_DIRECTORY = \"/content/drive/My Drive/2020 Spring/RDS/rds-project/data\"#\"../input/\"\n",
    "SUBMISSION_SUFIX = \"_model2_04\"\n",
    "data_path = \"/content/drive/My Drive/2020 Spring/RDS/rds-project/data\"\n",
    "output_path = \"/content/drive/My Drive/2020 Spring/RDS/rds-project/output\"\n",
    "\n",
    "# INSTALLMENTS TREND PERIODS\n",
    "INSTALLMENTS_LAST_K_TREND_PERIODS =  [12, 24, 60, 120]\n",
    "\n",
    "# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n",
    "GENERATE_SUBMISSION_FILES = True\n",
    "STRATIFIED_KFOLD = False\n",
    "RANDOM_SEED = 737851\n",
    "NUM_FOLDS = 10\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "LIGHTGBM_PARAMS = {\n",
    "    'boosting_type': 'goss',\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.005134,\n",
    "    'num_leaves': 54,\n",
    "    'max_depth': 10,\n",
    "    'subsample_for_bin': 240000,\n",
    "    'reg_alpha': 0.436193,\n",
    "    'reg_lambda': 0.479169,\n",
    "    'colsample_bytree': 0.508716,\n",
    "    'min_split_gain': 0.024766,\n",
    "    'subsample': 1,\n",
    "    'is_unbalance': False,\n",
    "    'silent':-1,\n",
    "    'verbose':-1\n",
    "}\n",
    "\n",
    "# AGGREGATIONS\n",
    "BUREAU_AGG = {\n",
    "    'SK_ID_BUREAU': ['nunique'],\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean', 'sum'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "    # Categorical\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_1': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "    'STATUS_C': ['mean'],\n",
    "    'STATUS_X': ['mean'],\n",
    "    'CREDIT_ACTIVE_Active': ['mean'],\n",
    "    'CREDIT_ACTIVE_Closed': ['mean'],\n",
    "    'CREDIT_ACTIVE_Sold': ['mean'],\n",
    "    'CREDIT_TYPE_Consumer credit': ['mean'],\n",
    "    'CREDIT_TYPE_Credit card': ['mean'],\n",
    "    'CREDIT_TYPE_Car loan': ['mean'],\n",
    "    'CREDIT_TYPE_Mortgage': ['mean'],\n",
    "    'CREDIT_TYPE_Microloan': ['mean'],\n",
    "    # Group by loan duration features (months)\n",
    "    'LL_AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'LL_DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'LL_STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "BUREAU_ACTIVE_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_UPDATE': ['min', 'mean'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "}\n",
    "\n",
    "BUREAU_CLOSED_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'var'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'sum'],\n",
    "    'DAYS_CREDIT_UPDATE': ['max'],\n",
    "    'ENDDATE_DIF': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "BUREAU_LOAN_TYPE_AGG = {\n",
    "    'DAYS_CREDIT': ['mean', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "    'AMT_CREDIT_SUM': ['mean', 'max'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'max'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "}\n",
    "\n",
    "BUREAU_TIME_AGG = {\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['max'],\n",
    "    # Engineered features\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean', 'var'],\n",
    "    'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_ACTIVE_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'SIMPLE_INTERESTS': ['mean'],\n",
    "    'AMT_ANNUITY': ['max', 'sum'],\n",
    "    'AMT_APPLICATION': ['max', 'mean'],\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'AMT_PAYMENT': ['sum'],\n",
    "    'INSTALMENT_PAYMENT_DIFF': ['mean', 'max'],\n",
    "    'REMAINING_DEBT': ['max', 'mean', 'sum'],\n",
    "    'REPAYMENT_RATIO': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_APPROVED_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': ['max'],\n",
    "    'AMT_GOODS_PRICE': ['max'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['mean'],\n",
    "    # Engineered features\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['max'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    # The following features are only for approved applications\n",
    "    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n",
    "    'DAYS_FIRST_DUE': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE': ['max', 'mean'],\n",
    "    'DAYS_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n",
    "    'SIMPLE_INTERESTS': ['min', 'max', 'mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_REFUSED_AGG = {\n",
    "    'AMT_APPLICATION': ['max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'var'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'mean'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_LATE_PAYMENTS_AGG = {\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_LOAN_TYPE_AGG = {\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_ANNUITY': ['mean', 'max'],\n",
    "    'SIMPLE_INTERESTS': ['min', 'mean', 'max', 'var'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'var'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    'DAYS_DECISION': ['max'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['max', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_TIME_AGG = {\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_ANNUITY': ['mean', 'max'],\n",
    "    'SIMPLE_INTERESTS': ['mean', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "POS_CASH_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'MONTHS_BALANCE': ['min', 'max', 'size'],\n",
    "    'SK_DPD': ['max', 'mean', 'sum', 'var'],\n",
    "    'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "    'LATE_PAYMENT': ['mean']\n",
    "}\n",
    "\n",
    "INSTALLMENTS_AGG = {\n",
    "    'SK_ID_PREV': ['size', 'nunique'],\n",
    "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'DPD': ['max', 'mean', 'var'],\n",
    "    'DBD': ['max', 'mean', 'var'],\n",
    "    'PAYMENT_DIFFERENCE': ['mean'],\n",
    "    'PAYMENT_RATIO': ['mean'],\n",
    "    'LATE_PAYMENT': ['mean', 'sum'],\n",
    "    'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "    'LATE_PAYMENT_RATIO': ['mean'],\n",
    "    'DPD_7': ['mean'],\n",
    "    'DPD_15': ['mean'],\n",
    "    'PAID_OVER': ['mean']\n",
    "}\n",
    "\n",
    "INSTALLMENTS_TIME_AGG = {\n",
    "    'SK_ID_PREV': ['size'],\n",
    "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'DPD': ['max', 'mean', 'var'],\n",
    "    'DBD': ['max', 'mean', 'var'],\n",
    "    'PAYMENT_DIFFERENCE': ['mean'],\n",
    "    'PAYMENT_RATIO': ['mean'],\n",
    "    'LATE_PAYMENT': ['mean'],\n",
    "    'SIGNIFICANT_LATE_PAYMENT': ['mean'],\n",
    "    'LATE_PAYMENT_RATIO': ['mean'],\n",
    "    'DPD_7': ['mean'],\n",
    "    'DPD_15': ['mean'],\n",
    "}\n",
    "\n",
    "CREDIT_CARD_AGG = {\n",
    "    'MONTHS_BALANCE': ['min'],\n",
    "    'AMT_BALANCE': ['max'],\n",
    "    'AMT_CREDIT_LIMIT_ACTUAL': ['max'],\n",
    "    'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "    'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "    'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "    'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "    'AMT_PAYMENT_TOTAL_CURRENT': ['max', 'mean', 'sum', 'var'],\n",
    "    'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "    'CNT_DRAWINGS_ATM_CURRENT': ['max', 'mean', 'sum'],\n",
    "    'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "    'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "    'SK_DPD': ['mean', 'max', 'sum'],\n",
    "    'SK_DPD_DEF': ['max', 'sum'],\n",
    "    'LIMIT_USE': ['max', 'mean'],\n",
    "    'PAYMENT_DIV_MIN': ['min', 'mean'],\n",
    "    'LATE_PAYMENT': ['max', 'sum'],\n",
    "}\n",
    "\n",
    "CREDIT_CARD_TIME_AGG = {\n",
    "    'CNT_DRAWINGS_ATM_CURRENT': ['mean'],\n",
    "    'SK_DPD': ['max', 'sum'],\n",
    "    'AMT_BALANCE': ['mean', 'max'],\n",
    "    'LIMIT_USE': ['max', 'mean']\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UYVHl4bFyXb"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8152374,
     "status": "ok",
     "timestamp": 1585943618424,
     "user": {
      "displayName": "Eileen Cho",
      "photoUrl": "",
      "userId": "03381570147993013394"
     },
     "user_tz": 240
    },
    "id": "XAmSSoI2W-g5",
    "outputId": "8bf7b58f-1ba6-4cf4-be60-902261a5e455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered df shape: (356250, 660)\n",
      "Loading engineered features - done in 39s\n",
      "Train/valid shape: (307506, 660), test shape: (48744, 660)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801414\ttraining's binary_logloss: 0.236853\tvalid_1's auc: 0.766387\tvalid_1's binary_logloss: 0.245158\n",
      "[800]\ttraining's auc: 0.828831\ttraining's binary_logloss: 0.224187\tvalid_1's auc: 0.780502\tvalid_1's binary_logloss: 0.239113\n",
      "[1200]\ttraining's auc: 0.848118\ttraining's binary_logloss: 0.215666\tvalid_1's auc: 0.786374\tvalid_1's binary_logloss: 0.236771\n",
      "[1600]\ttraining's auc: 0.863991\ttraining's binary_logloss: 0.208707\tvalid_1's auc: 0.789732\tvalid_1's binary_logloss: 0.235555\n",
      "[2000]\ttraining's auc: 0.877696\ttraining's binary_logloss: 0.202566\tvalid_1's auc: 0.79137\tvalid_1's binary_logloss: 0.23495\n",
      "[2400]\ttraining's auc: 0.890001\ttraining's binary_logloss: 0.196909\tvalid_1's auc: 0.792557\tvalid_1's binary_logloss: 0.234542\n",
      "[2800]\ttraining's auc: 0.901201\ttraining's binary_logloss: 0.19166\tvalid_1's auc: 0.793553\tvalid_1's binary_logloss: 0.234195\n",
      "[3200]\ttraining's auc: 0.910901\ttraining's binary_logloss: 0.186781\tvalid_1's auc: 0.794282\tvalid_1's binary_logloss: 0.233969\n",
      "[3600]\ttraining's auc: 0.919643\ttraining's binary_logloss: 0.182168\tvalid_1's auc: 0.794655\tvalid_1's binary_logloss: 0.23383\n",
      "[4000]\ttraining's auc: 0.927616\ttraining's binary_logloss: 0.17778\tvalid_1's auc: 0.794967\tvalid_1's binary_logloss: 0.233726\n",
      "[4400]\ttraining's auc: 0.934693\ttraining's binary_logloss: 0.173604\tvalid_1's auc: 0.795458\tvalid_1's binary_logloss: 0.233593\n",
      "[4800]\ttraining's auc: 0.941081\ttraining's binary_logloss: 0.169616\tvalid_1's auc: 0.795739\tvalid_1's binary_logloss: 0.233512\n",
      "Early stopping, best iteration is:\n",
      "[4854]\ttraining's auc: 0.941886\ttraining's binary_logloss: 0.169084\tvalid_1's auc: 0.795788\tvalid_1's binary_logloss: 0.233503\n",
      "Fold  1 AUC : 0.795788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801393\ttraining's binary_logloss: 0.236489\tvalid_1's auc: 0.768559\tvalid_1's binary_logloss: 0.248143\n",
      "[800]\ttraining's auc: 0.828865\ttraining's binary_logloss: 0.223849\tvalid_1's auc: 0.781926\tvalid_1's binary_logloss: 0.241925\n",
      "[1200]\ttraining's auc: 0.848148\ttraining's binary_logloss: 0.21535\tvalid_1's auc: 0.788273\tvalid_1's binary_logloss: 0.239341\n",
      "[1600]\ttraining's auc: 0.863969\ttraining's binary_logloss: 0.208404\tvalid_1's auc: 0.791589\tvalid_1's binary_logloss: 0.238039\n",
      "[2000]\ttraining's auc: 0.877824\ttraining's binary_logloss: 0.202247\tvalid_1's auc: 0.793294\tvalid_1's binary_logloss: 0.237336\n",
      "[2400]\ttraining's auc: 0.890115\ttraining's binary_logloss: 0.196629\tvalid_1's auc: 0.794683\tvalid_1's binary_logloss: 0.236799\n",
      "[2800]\ttraining's auc: 0.901186\ttraining's binary_logloss: 0.191413\tvalid_1's auc: 0.795585\tvalid_1's binary_logloss: 0.236472\n",
      "[3200]\ttraining's auc: 0.911022\ttraining's binary_logloss: 0.18652\tvalid_1's auc: 0.796114\tvalid_1's binary_logloss: 0.236267\n",
      "[3600]\ttraining's auc: 0.919986\ttraining's binary_logloss: 0.181881\tvalid_1's auc: 0.796517\tvalid_1's binary_logloss: 0.236098\n",
      "[4000]\ttraining's auc: 0.927923\ttraining's binary_logloss: 0.177496\tvalid_1's auc: 0.796751\tvalid_1's binary_logloss: 0.236009\n",
      "Early stopping, best iteration is:\n",
      "[3996]\ttraining's auc: 0.927863\ttraining's binary_logloss: 0.177536\tvalid_1's auc: 0.796755\tvalid_1's binary_logloss: 0.236008\n",
      "Fold  2 AUC : 0.796755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801008\ttraining's binary_logloss: 0.237614\tvalid_1's auc: 0.77272\tvalid_1's binary_logloss: 0.238203\n",
      "[800]\ttraining's auc: 0.828478\ttraining's binary_logloss: 0.224932\tvalid_1's auc: 0.787124\tvalid_1's binary_logloss: 0.231975\n",
      "[1200]\ttraining's auc: 0.847612\ttraining's binary_logloss: 0.216422\tvalid_1's auc: 0.793648\tvalid_1's binary_logloss: 0.229419\n",
      "[1600]\ttraining's auc: 0.863459\ttraining's binary_logloss: 0.209429\tvalid_1's auc: 0.796991\tvalid_1's binary_logloss: 0.228084\n",
      "[2000]\ttraining's auc: 0.877348\ttraining's binary_logloss: 0.203243\tvalid_1's auc: 0.799143\tvalid_1's binary_logloss: 0.227253\n",
      "[2400]\ttraining's auc: 0.88982\ttraining's binary_logloss: 0.197573\tvalid_1's auc: 0.80043\tvalid_1's binary_logloss: 0.22672\n",
      "[2800]\ttraining's auc: 0.901036\ttraining's binary_logloss: 0.192294\tvalid_1's auc: 0.801041\tvalid_1's binary_logloss: 0.226427\n",
      "[3200]\ttraining's auc: 0.910691\ttraining's binary_logloss: 0.187421\tvalid_1's auc: 0.801557\tvalid_1's binary_logloss: 0.22622\n",
      "[3600]\ttraining's auc: 0.91948\ttraining's binary_logloss: 0.182794\tvalid_1's auc: 0.80208\tvalid_1's binary_logloss: 0.226005\n",
      "Early stopping, best iteration is:\n",
      "[3604]\ttraining's auc: 0.919566\ttraining's binary_logloss: 0.182749\tvalid_1's auc: 0.802105\tvalid_1's binary_logloss: 0.225996\n",
      "Fold  3 AUC : 0.802106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801205\ttraining's binary_logloss: 0.236282\tvalid_1's auc: 0.771703\tvalid_1's binary_logloss: 0.250207\n",
      "[800]\ttraining's auc: 0.828626\ttraining's binary_logloss: 0.223672\tvalid_1's auc: 0.786453\tvalid_1's binary_logloss: 0.243519\n",
      "[1200]\ttraining's auc: 0.847716\ttraining's binary_logloss: 0.21524\tvalid_1's auc: 0.792672\tvalid_1's binary_logloss: 0.240859\n",
      "[1600]\ttraining's auc: 0.863867\ttraining's binary_logloss: 0.208253\tvalid_1's auc: 0.796221\tvalid_1's binary_logloss: 0.239437\n",
      "[2000]\ttraining's auc: 0.877691\ttraining's binary_logloss: 0.202083\tvalid_1's auc: 0.798499\tvalid_1's binary_logloss: 0.238573\n",
      "[2400]\ttraining's auc: 0.890121\ttraining's binary_logloss: 0.19643\tvalid_1's auc: 0.799803\tvalid_1's binary_logloss: 0.238067\n",
      "[2800]\ttraining's auc: 0.901261\ttraining's binary_logloss: 0.191192\tvalid_1's auc: 0.80041\tvalid_1's binary_logloss: 0.237797\n",
      "[3200]\ttraining's auc: 0.91112\ttraining's binary_logloss: 0.186319\tvalid_1's auc: 0.800853\tvalid_1's binary_logloss: 0.237592\n",
      "[3600]\ttraining's auc: 0.919852\ttraining's binary_logloss: 0.1817\tvalid_1's auc: 0.801491\tvalid_1's binary_logloss: 0.237368\n",
      "[4000]\ttraining's auc: 0.927708\ttraining's binary_logloss: 0.177338\tvalid_1's auc: 0.801529\tvalid_1's binary_logloss: 0.237326\n",
      "Early stopping, best iteration is:\n",
      "[3904]\ttraining's auc: 0.925866\ttraining's binary_logloss: 0.178365\tvalid_1's auc: 0.801639\tvalid_1's binary_logloss: 0.237296\n",
      "Fold  4 AUC : 0.801639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801295\ttraining's binary_logloss: 0.236445\tvalid_1's auc: 0.765091\tvalid_1's binary_logloss: 0.248904\n",
      "[800]\ttraining's auc: 0.828925\ttraining's binary_logloss: 0.223777\tvalid_1's auc: 0.779107\tvalid_1's binary_logloss: 0.24292\n",
      "[1200]\ttraining's auc: 0.848379\ttraining's binary_logloss: 0.215252\tvalid_1's auc: 0.785285\tvalid_1's binary_logloss: 0.240559\n",
      "[1600]\ttraining's auc: 0.864072\ttraining's binary_logloss: 0.208273\tvalid_1's auc: 0.789053\tvalid_1's binary_logloss: 0.239246\n",
      "[2000]\ttraining's auc: 0.877796\ttraining's binary_logloss: 0.202135\tvalid_1's auc: 0.791039\tvalid_1's binary_logloss: 0.238517\n",
      "[2400]\ttraining's auc: 0.890165\ttraining's binary_logloss: 0.196527\tvalid_1's auc: 0.792382\tvalid_1's binary_logloss: 0.238064\n",
      "[2800]\ttraining's auc: 0.901241\ttraining's binary_logloss: 0.191292\tvalid_1's auc: 0.793088\tvalid_1's binary_logloss: 0.23782\n",
      "[3200]\ttraining's auc: 0.910929\ttraining's binary_logloss: 0.186439\tvalid_1's auc: 0.793742\tvalid_1's binary_logloss: 0.237589\n",
      "[3600]\ttraining's auc: 0.91983\ttraining's binary_logloss: 0.181821\tvalid_1's auc: 0.794323\tvalid_1's binary_logloss: 0.237406\n",
      "[4000]\ttraining's auc: 0.927807\ttraining's binary_logloss: 0.177425\tvalid_1's auc: 0.794808\tvalid_1's binary_logloss: 0.237238\n",
      "[4400]\ttraining's auc: 0.934923\ttraining's binary_logloss: 0.173261\tvalid_1's auc: 0.795118\tvalid_1's binary_logloss: 0.237143\n",
      "[4800]\ttraining's auc: 0.94134\ttraining's binary_logloss: 0.16926\tvalid_1's auc: 0.79527\tvalid_1's binary_logloss: 0.23709\n",
      "Early stopping, best iteration is:\n",
      "[4968]\ttraining's auc: 0.943816\ttraining's binary_logloss: 0.167649\tvalid_1's auc: 0.795416\tvalid_1's binary_logloss: 0.237034\n",
      "Fold  5 AUC : 0.795414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.800651\ttraining's binary_logloss: 0.236788\tvalid_1's auc: 0.774321\tvalid_1's binary_logloss: 0.245741\n",
      "[800]\ttraining's auc: 0.828283\ttraining's binary_logloss: 0.224183\tvalid_1's auc: 0.788919\tvalid_1's binary_logloss: 0.239077\n",
      "[1200]\ttraining's auc: 0.8476\ttraining's binary_logloss: 0.215678\tvalid_1's auc: 0.795481\tvalid_1's binary_logloss: 0.23638\n",
      "[1600]\ttraining's auc: 0.863355\ttraining's binary_logloss: 0.20874\tvalid_1's auc: 0.799076\tvalid_1's binary_logloss: 0.234949\n",
      "[2000]\ttraining's auc: 0.877255\ttraining's binary_logloss: 0.202583\tvalid_1's auc: 0.801062\tvalid_1's binary_logloss: 0.23411\n",
      "[2400]\ttraining's auc: 0.889516\ttraining's binary_logloss: 0.196967\tvalid_1's auc: 0.802312\tvalid_1's binary_logloss: 0.233595\n",
      "[2800]\ttraining's auc: 0.900411\ttraining's binary_logloss: 0.191766\tvalid_1's auc: 0.803151\tvalid_1's binary_logloss: 0.233254\n",
      "[3200]\ttraining's auc: 0.910342\ttraining's binary_logloss: 0.186871\tvalid_1's auc: 0.803652\tvalid_1's binary_logloss: 0.233025\n",
      "[3600]\ttraining's auc: 0.919229\ttraining's binary_logloss: 0.182276\tvalid_1's auc: 0.804317\tvalid_1's binary_logloss: 0.232772\n",
      "[4000]\ttraining's auc: 0.92711\ttraining's binary_logloss: 0.177897\tvalid_1's auc: 0.804809\tvalid_1's binary_logloss: 0.232596\n",
      "Early stopping, best iteration is:\n",
      "[4140]\ttraining's auc: 0.929694\ttraining's binary_logloss: 0.176406\tvalid_1's auc: 0.80491\tvalid_1's binary_logloss: 0.232551\n",
      "Fold  6 AUC : 0.804912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801277\ttraining's binary_logloss: 0.236994\tvalid_1's auc: 0.77474\tvalid_1's binary_logloss: 0.243692\n",
      "[800]\ttraining's auc: 0.828885\ttraining's binary_logloss: 0.224274\tvalid_1's auc: 0.787892\tvalid_1's binary_logloss: 0.237348\n",
      "[1200]\ttraining's auc: 0.848101\ttraining's binary_logloss: 0.215783\tvalid_1's auc: 0.793731\tvalid_1's binary_logloss: 0.234747\n",
      "[1600]\ttraining's auc: 0.863755\ttraining's binary_logloss: 0.208833\tvalid_1's auc: 0.796698\tvalid_1's binary_logloss: 0.233448\n",
      "[2000]\ttraining's auc: 0.877463\ttraining's binary_logloss: 0.202696\tvalid_1's auc: 0.798494\tvalid_1's binary_logloss: 0.232699\n",
      "[2400]\ttraining's auc: 0.889718\ttraining's binary_logloss: 0.197099\tvalid_1's auc: 0.799441\tvalid_1's binary_logloss: 0.232259\n",
      "[2800]\ttraining's auc: 0.900701\ttraining's binary_logloss: 0.191869\tvalid_1's auc: 0.80023\tvalid_1's binary_logloss: 0.231923\n",
      "[3200]\ttraining's auc: 0.910412\ttraining's binary_logloss: 0.187029\tvalid_1's auc: 0.800717\tvalid_1's binary_logloss: 0.231726\n",
      "Early stopping, best iteration is:\n",
      "[3437]\ttraining's auc: 0.915692\ttraining's binary_logloss: 0.184273\tvalid_1's auc: 0.8009\tvalid_1's binary_logloss: 0.231642\n",
      "Fold  7 AUC : 0.800885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.801693\ttraining's binary_logloss: 0.236477\tvalid_1's auc: 0.768375\tvalid_1's binary_logloss: 0.247882\n",
      "[800]\ttraining's auc: 0.829048\ttraining's binary_logloss: 0.223869\tvalid_1's auc: 0.782807\tvalid_1's binary_logloss: 0.241591\n",
      "[1200]\ttraining's auc: 0.848151\ttraining's binary_logloss: 0.215399\tvalid_1's auc: 0.789307\tvalid_1's binary_logloss: 0.239029\n",
      "[1600]\ttraining's auc: 0.863865\ttraining's binary_logloss: 0.208457\tvalid_1's auc: 0.792647\tvalid_1's binary_logloss: 0.237785\n",
      "[2000]\ttraining's auc: 0.877483\ttraining's binary_logloss: 0.202298\tvalid_1's auc: 0.794644\tvalid_1's binary_logloss: 0.237073\n",
      "[2400]\ttraining's auc: 0.889854\ttraining's binary_logloss: 0.196674\tvalid_1's auc: 0.795898\tvalid_1's binary_logloss: 0.23658\n",
      "[2800]\ttraining's auc: 0.900855\ttraining's binary_logloss: 0.191467\tvalid_1's auc: 0.796901\tvalid_1's binary_logloss: 0.236215\n",
      "[3200]\ttraining's auc: 0.910686\ttraining's binary_logloss: 0.18659\tvalid_1's auc: 0.797729\tvalid_1's binary_logloss: 0.235945\n",
      "[3600]\ttraining's auc: 0.91948\ttraining's binary_logloss: 0.181996\tvalid_1's auc: 0.798034\tvalid_1's binary_logloss: 0.235828\n",
      "Early stopping, best iteration is:\n",
      "[3525]\ttraining's auc: 0.917893\ttraining's binary_logloss: 0.182846\tvalid_1's auc: 0.798121\tvalid_1's binary_logloss: 0.235804\n",
      "Fold  8 AUC : 0.798122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.800623\ttraining's binary_logloss: 0.237279\tvalid_1's auc: 0.777305\tvalid_1's binary_logloss: 0.241389\n",
      "[800]\ttraining's auc: 0.828441\ttraining's binary_logloss: 0.224566\tvalid_1's auc: 0.78946\tvalid_1's binary_logloss: 0.235278\n",
      "[1200]\ttraining's auc: 0.847791\ttraining's binary_logloss: 0.216036\tvalid_1's auc: 0.794253\tvalid_1's binary_logloss: 0.232926\n",
      "[1600]\ttraining's auc: 0.863855\ttraining's binary_logloss: 0.209051\tvalid_1's auc: 0.796953\tvalid_1's binary_logloss: 0.231727\n",
      "[2000]\ttraining's auc: 0.877504\ttraining's binary_logloss: 0.202878\tvalid_1's auc: 0.798503\tvalid_1's binary_logloss: 0.231029\n",
      "[2400]\ttraining's auc: 0.889987\ttraining's binary_logloss: 0.197217\tvalid_1's auc: 0.799635\tvalid_1's binary_logloss: 0.230566\n",
      "[2800]\ttraining's auc: 0.901119\ttraining's binary_logloss: 0.191959\tvalid_1's auc: 0.800389\tvalid_1's binary_logloss: 0.230223\n",
      "[3200]\ttraining's auc: 0.910934\ttraining's binary_logloss: 0.187062\tvalid_1's auc: 0.800852\tvalid_1's binary_logloss: 0.230004\n",
      "[3600]\ttraining's auc: 0.919637\ttraining's binary_logloss: 0.182464\tvalid_1's auc: 0.801216\tvalid_1's binary_logloss: 0.229864\n",
      "[4000]\ttraining's auc: 0.927545\ttraining's binary_logloss: 0.178076\tvalid_1's auc: 0.801455\tvalid_1's binary_logloss: 0.229751\n",
      "Early stopping, best iteration is:\n",
      "[3949]\ttraining's auc: 0.926575\ttraining's binary_logloss: 0.178626\tvalid_1's auc: 0.801493\tvalid_1's binary_logloss: 0.229742\n",
      "Fold  9 AUC : 0.801496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[400]\ttraining's auc: 0.800431\ttraining's binary_logloss: 0.237765\tvalid_1's auc: 0.780193\tvalid_1's binary_logloss: 0.237128\n",
      "[800]\ttraining's auc: 0.828496\ttraining's binary_logloss: 0.224995\tvalid_1's auc: 0.790821\tvalid_1's binary_logloss: 0.231357\n",
      "[1200]\ttraining's auc: 0.847407\ttraining's binary_logloss: 0.216503\tvalid_1's auc: 0.795451\tvalid_1's binary_logloss: 0.22923\n",
      "[1600]\ttraining's auc: 0.863305\ttraining's binary_logloss: 0.209513\tvalid_1's auc: 0.798276\tvalid_1's binary_logloss: 0.228081\n",
      "[2000]\ttraining's auc: 0.877081\ttraining's binary_logloss: 0.203351\tvalid_1's auc: 0.799914\tvalid_1's binary_logloss: 0.22746\n",
      "[2400]\ttraining's auc: 0.889586\ttraining's binary_logloss: 0.197685\tvalid_1's auc: 0.800846\tvalid_1's binary_logloss: 0.227091\n",
      "[2800]\ttraining's auc: 0.900617\ttraining's binary_logloss: 0.192427\tvalid_1's auc: 0.801482\tvalid_1's binary_logloss: 0.226829\n",
      "Early stopping, best iteration is:\n",
      "[2925]\ttraining's auc: 0.903829\ttraining's binary_logloss: 0.190856\tvalid_1's auc: 0.801724\tvalid_1's binary_logloss: 0.226734\n",
      "Fold 10 AUC : 0.801720\n",
      "Full AUC score 0.799790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          feature           gain   split\n",
      "280                              EXT_SOURCES_MEAN  712154.933583  1735.3\n",
      "282                         EXT_SOURCES_NANMEDIAN  314792.401939  1323.3\n",
      "434                             ORGANIZATION_TYPE  179835.939985  6980.7\n",
      "281                               EXT_SOURCES_MIN  149484.699400  1335.0\n",
      "254                       CREDIT_TO_ANNUITY_RATIO   94106.385027  2379.0\n",
      "..                                            ...            ...     ...\n",
      "498             PREV_CHANNEL_TYPE_Car dealer_MEAN      14.957780     0.6\n",
      "241                           CC_LATE_PAYMENT_MAX       7.514410     0.5\n",
      "39                APPROVED_DAYS_FIRST_DRAWING_MAX       6.060730     0.2\n",
      "183  BUREAU_MICROLOAN_AMT_CREDIT_MAX_OVERDUE_MEAN       1.518360     0.1\n",
      "182   BUREAU_MICROLOAN_AMT_CREDIT_MAX_OVERDUE_MAX       1.353740     0.1\n",
      "\n",
      "[658 rows x 3 columns]\n",
      "Run LightGBM - done in 10418s\n",
      "Pipeline total time - done in 10458s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pd.set_option('display.max_rows', 60)\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "    with timer(\"Pipeline total time\"):\n",
    "        main(debug= False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO1avIlnOQ9y1V5Wd3seQqO",
   "collapsed_sections": [
    "HSeeUzEyFjvv",
    "cJFBPuZhFusU",
    "7UYVHl4bFyXb"
   ],
   "machine_shape": "hm",
   "mount_file_id": "1sJi0wNhSckhWrqVA0d1M3BbT2Md6B9PH",
   "name": "HomeCredit_ADS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
